#!/usr/bin/env python3
"""
æ”¿æ²»ãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åé›†Botï¼ˆä¸–è«–åˆ†ææ©Ÿèƒ½ä»˜ãï¼‰
æ¯æ™‚ã€è¤‡æ•°ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æ”¿æ²»é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã€Discordã«æŠ•ç¨¿
Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’åˆ†æã—ã¦ä¸–è«–ã®å‚¾å‘ã‚‚è¡¨ç¤º
"""

import os
import sys
import re
import time
from datetime import datetime
from typing import List, Dict, Optional
import feedparser
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai

# ç’°å¢ƒå¤‰æ•°ã®å–å¾—
DISCORD_WEBHOOK_URL = os.environ.get('DISCORD_WEBHOOK_POLITICS')
GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')

# Gemini APIè¨­å®š
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-1.5-flash')

# æ”¿æ²»é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ3æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ï¼‰
POLITICAL_KEYWORDS = [
    # æ”¿å…š
    'è‡ªæ°‘', 'å›½æ°‘æ°‘ä¸»', 'å‚æ”¿', 'ç¶­æ–°', 'ç«‹æ†²', 'å…±ç”£', 'å…¬æ˜', 'ç¤¾æ°‘', 'ä»¤å’Œ', 'ã‚Œã„ã‚',
    # æ”¿æ²»å®¶
    'é«˜å¸‚', 'éº»ç”Ÿ', 'ç‰‡å±±', 'å°é‡ç”°', 'èŒ‚æœ¨', 'éˆ´æœ¨ä¿Šä¸€', 'å°¾å´', 'çŸ³åŸ', 'å®‰å€æ™‹ä¸‰', 'ä¸‰æ—¥æœˆ',
    'å²¸ç”°', 'æ²³é‡', 'çŸ³ç ´', 'å°æ³‰', 'é‡ç”°', 'æé‡', 'å¿—ä½', 'ç‰æœ¨', 'é¦¬å ´',
    # å½¹è·
    'é¦–ç›¸', 'ç·ç†', 'å¤§è‡£', 'å®˜æˆ¿é•·å®˜', 'è²¡å‹™å¤§è‡£', 'å¤–ç›¸', 'é˜²è¡›ç›¸', 'è¾²æ°´ç›¸', 'ç’°å¢ƒç›¸',
    'åšåŠ´ç›¸', 'æ–‡ç§‘ç›¸', 'çµŒç”£ç›¸', 'å›½äº¤ç›¸', 'ç·å‹™ç›¸', 'æ³•ç›¸', 'å†…é–£åºœ',
    # æ”¿ç­–ãƒ»åˆ¶åº¦
    'å¢—ç¨', 'æ¸›ç¨', 'é˜²è¡›è²»', 'ç¤¾ä¼šä¿éšœ', 'è²¡æº', 'æ†²æ³•æ”¹æ­£', 'å®‰å…¨ä¿éšœ', 'é–¢ç¨', 'è²¿æ˜“',
    'å°‘å­åŒ–å¯¾ç­–', 'å¹´é‡‘æ”¹é©', 'æ§é™¤', 'çµ¦ä»˜é‡‘', 'è£œåŠ©é‡‘', 'äºˆç®—', 'ç¨åˆ¶',
    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ»åˆ¶åº¦
    'å›½ä¼š', 'äºˆç®—å§”å“¡ä¼š', 'å…šé¦–è¨è«–', 'é¸æŒ™', 'å†…é–£æ”¹é€ ', 'è§£æ•£', 'ä¸ä¿¡ä»»', 'æ”¿æ²»è³‡é‡‘',
    'æ”¿æ²»çŒ®é‡‘', 'æ”¯æŒç‡', 'è¡†é™¢é¸', 'å‚é™¢é¸', 'è£œé¸', 'åœ°æ–¹é¸',
    # å›½éš›æ”¿æ²»
    'æ—¥ç±³', 'æ—¥ä¸­', 'æ—¥éŸ“', 'ç±³ä¸­', 'ç±³éœ²', 'ç±³ãƒ­', 'G7', 'G20', 'å›½é€£', 'ASEAN',
    'ä¼šè«‡', 'é¦–è„³', 'ã‚µãƒŸãƒƒãƒˆ', 'ãƒˆãƒ©ãƒ³ãƒ—', 'ãƒ—ãƒ¼ãƒãƒ³', 'ç¿’è¿‘å¹³', 'ãƒã‚¤ãƒ‡ãƒ³'
]

# é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚¹ãƒãƒ¼ãƒ„ãƒ»èŠ¸èƒ½ç­‰ï¼‰
EXCLUDE_KEYWORDS = [
    # ã‚¹ãƒãƒ¼ãƒ„
    'ãƒ—ãƒ­ãƒ¬ã‚¹', 'æ–°æ—¥æœ¬ãƒ—ãƒ­ãƒ¬ã‚¹', 'ã‚µãƒƒã‚«ãƒ¼', 'é‡çƒ', 'ãƒã‚¹ã‚±', 'ãƒ†ãƒ‹ã‚¹', 'ã‚´ãƒ«ãƒ•',
    'æ ¼é—˜æŠ€', 'ãƒœã‚¯ã‚·ãƒ³ã‚°', 'ãƒ¬ã‚¹ãƒªãƒ³ã‚°', 'äº”è¼ª', 'ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯', 'Wæ¯',
    # èŠ¸èƒ½
    'æ˜ ç”»', 'ãƒ‰ãƒ©ãƒ', 'ã‚¢ã‚¤ãƒ‰ãƒ«', 'ä¿³å„ª', 'å¥³å„ª', 'ã‚¿ãƒ¬ãƒ³ãƒˆ', 'ãƒŸãƒ¥ãƒ¼ã‚¸ã‚·ãƒ£ãƒ³',
    'æ­Œæ‰‹', 'ãƒãƒ³ãƒ‰', 'ã‚¢ãƒ‹ãƒ¡', 'ã‚²ãƒ¼ãƒ ',
    # ãã®ä»–
    'éˆ´æœ¨ã¿ã®ã‚‹', 'éˆ´æœ¨è»'  # èª¤æ¤œå‡ºå¯¾ç­–
]

# RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®URL
NEWS_FEEDS = {
    'æ—¥æœ¬çµŒæ¸ˆæ–°è': 'https://www.nikkei.com/rss/',
    'èª­å£²æ–°è': 'https://www.yomiuri.co.jp/rss/l-news.xml',
    'æœæ—¥æ–°è': 'https://www.asahi.com/rss/asahi/newsheadlines.rdf',
    'æ¯æ—¥æ–°è': 'https://mainichi.jp/rss/etc/mainichi-flash.rss',
    'NHK': 'https://www.nhk.or.jp/rss/news/cat0.xml',
    'BBC News': 'https://feeds.bbci.co.uk/news/world/rss.xml',
    'Reuters': 'https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best',
    'CNN': 'http://rss.cnn.com/rss/edition_world.rss'
}


def check_political_relevance(title: str, description: str) -> int:
    """
    Gemini APIã‚’ä½¿ç”¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ”¿æ²»é–¢é€£åº¦ã‚’åˆ¤å®š
    
    Args:
        title: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«
        description: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®èª¬æ˜æ–‡
    
    Returns:
        æ”¿æ²»é–¢é€£åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0-100ç‚¹ï¼‰
    """
    if not GEMINI_API_KEY:
        return 0
    
    try:
        prompt = f"""
ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã€Œæ—¥æœ¬ã®æ”¿æ²»ã€ã«ã©ã‚Œã ã‘é–¢é€£ã—ã¦ã„ã‚‹ã‹ã€0ã€œ100ç‚¹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

è©•ä¾¡åŸºæº–ï¼š
- 90-100ç‚¹: æ—¥æœ¬ã®æ”¿æ²»ãƒ»æ”¿ç­–ã®ä¸­æ ¸çš„ãªè©±é¡Œï¼ˆé¸æŒ™ã€å›½ä¼šã€å†…é–£ã€æ³•æ¡ˆã€æ”¿æ²»å®¶ã®é‡è¦ç™ºè¨€ç­‰ï¼‰
- 70-89ç‚¹: æ—¥æœ¬ã®æ”¿æ²»ã«ç›´æ¥é–¢ä¿‚ã™ã‚‹è©±é¡Œï¼ˆæ”¿åºœã®æ”¿ç­–æ±ºå®šã€æ”¿æ²»è³‡é‡‘ã€æ”¯æŒç‡ç­‰ï¼‰
- 50-69ç‚¹: æ”¿æ²»ã¨é–¢é€£ãŒã‚ã‚‹ãŒé–“æ¥çš„ï¼ˆçµŒæ¸ˆæ”¿ç­–ã®å½±éŸ¿ã€å›½éš›é–¢ä¿‚ç­‰ï¼‰
- 30-49ç‚¹: æ”¿æ²»çš„è¦ç´ ã‚’å«ã‚€ãŒä¸»é¡Œã§ã¯ãªã„
- 0-29ç‚¹: æ”¿æ²»ã¨ã»ã¼ç„¡é–¢ä¿‚ï¼ˆã‚¹ãƒãƒ¼ãƒ„ã€èŠ¸èƒ½ã€ä¸€èˆ¬ãƒ‹ãƒ¥ãƒ¼ã‚¹ç­‰ï¼‰

ã‚¿ã‚¤ãƒˆãƒ«: {title}
èª¬æ˜: {description}

æ•°å­—ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆä¾‹: 85ï¼‰
"""
        
        response = model.generate_content(prompt)
        score_text = response.text.strip()
        
        # æ•°å­—ã®ã¿æŠ½å‡º
        score_match = re.search(r'\d+', score_text)
        if score_match:
            score = int(score_match.group())
            return min(100, max(0, score))  # 0-100ã«åˆ¶é™
        
        return 0
    
    except Exception as e:
        print(f"âš ï¸ Gemini APIåˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
        return 0


def filter_political_news(entries: List[Dict]) -> List[Dict]:
    """
    3æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§æ”¿æ²»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
    
    1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒï¼ˆé«˜é€Ÿï¼‰
    2. é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆèª¤æ¤œå‡ºé™¤å»ï¼‰
    3. Gemini AIåˆ¤å®šï¼ˆç²¾åº¦å‘ä¸Šï¼‰
    """
    print(f"\nğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–‹å§‹: {len(entries)}ä»¶")
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ
    keyword_matched = []
    for entry in entries:
        title = entry.get('title', '')
        description = entry.get('description', '')
        combined = f"{title} {description}"
        
        if any(keyword in combined for keyword in POLITICAL_KEYWORDS):
            keyword_matched.append(entry)
    
    print(f"âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ: {len(keyword_matched)}ä»¶")
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    exclude_filtered = []
    for entry in keyword_matched:
        title = entry.get('title', '')
        description = entry.get('description', '')
        combined = f"{title} {description}"
        
        if not any(exclude in combined for exclude in EXCLUDE_KEYWORDS):
            exclude_filtered.append(entry)
    
    print(f"âœ… é™¤å¤–ãƒ¯ãƒ¼ãƒ‰å¾Œ: {len(exclude_filtered)}ä»¶")
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: Gemini AIåˆ¤å®š
    political_news = []
    for entry in exclude_filtered:
        title = entry.get('title', '')
        description = entry.get('description', '')
        
        score = check_political_relevance(title, description)
        entry['political_score'] = score
        
        if score >= 70:  # 70ç‚¹ä»¥ä¸Šã®ã¿é€šé
            political_news.append(entry)
            print(f"  âœ… [{score}ç‚¹] {title}")
        else:
            print(f"  âŒ [{score}ç‚¹] {title}")
        
        time.sleep(0.5)  # APIåˆ¶é™å¯¾ç­–
    
    print(f"âœ… æœ€çµ‚çµæœ: {len(political_news)}ä»¶\n")
    return political_news


def search_yahoo_news(title: str) -> Optional[str]:
    """
    Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢ã—ã¦URLã‚’å–å¾—
    
    Args:
        title: æ¤œç´¢ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«
    
    Returns:
        Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®URLï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯Noneï¼‰
    """
    try:
        search_url = f"https://news.yahoo.co.jp/search?p={requests.utils.quote(title)}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(search_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'lxml')
        
        # æ¤œç´¢çµæœã®æœ€åˆã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
        first_result = soup.select_one('a[href*="news.yahoo.co.jp/articles/"]')
        if first_result:
            return first_result['href']
        
        return None
    
    except Exception as e:
        print(f"âš ï¸ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def get_yahoo_comments(article_url: str, max_comments: int = 100) -> List[Dict]:
    """
    Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
    
    Args:
        article_url: Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®URL
        max_comments: å–å¾—ã™ã‚‹æœ€å¤§ã‚³ãƒ¡ãƒ³ãƒˆæ•°
    
    Returns:
        ã‚³ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(article_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'lxml')
        
        comments = []
        comment_elements = soup.select('.comment-item')[:max_comments]
        
        for elem in comment_elements:
            text_elem = elem.select_one('.comment-text')
            likes_elem = elem.select_one('.likes-count')
            
            if text_elem:
                comment = {
                    'text': text_elem.get_text(strip=True),
                    'likes': int(likes_elem.get_text(strip=True)) if likes_elem else 0
                }
                comments.append(comment)
        
        return comments
    
    except Exception as e:
        print(f"âš ï¸ ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def analyze_sentiment(comments: List[Dict]) -> Dict:
    """
    Gemini APIã§ã‚³ãƒ¡ãƒ³ãƒˆã®æ„Ÿæƒ…åˆ†æ
    
    Args:
        comments: ã‚³ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
    
    Returns:
        åˆ†æçµæœ
    """
    if not comments or not GEMINI_API_KEY:
        return None
    
    try:
        # ä¸Šä½20ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’åˆ†æå¯¾è±¡ã«ã™ã‚‹
        top_comments = sorted(comments, key=lambda x: x['likes'], reverse=True)[:20]
        comments_text = "\n".join([f"- {c['text']}" for c in top_comments])
        
        prompt = f"""
ä»¥ä¸‹ã®Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¡ãƒ³ãƒˆã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

ã€ã‚³ãƒ¡ãƒ³ãƒˆä¸€è¦§ã€‘
{comments_text}

ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

1. æ„Ÿæƒ…åˆ†å¸ƒï¼ˆ%ã§è¡¨è¨˜ï¼‰
è³›æˆ: XX%
åå¯¾: XX%
ä¸­ç«‹: XX%

2. è­°è«–ã®ç†±é‡ï¼ˆ0-100ç‚¹ï¼‰
XXç‚¹

3. ä¸»è¦è«–ç‚¹ï¼ˆ3ã¤ï¼‰
- è«–ç‚¹1
- è«–ç‚¹2
- è«–ç‚¹3

4. ä»£è¡¨çš„ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆè³›æˆãƒ»åå¯¾ã‹ã‚‰å„1ã¤ï¼‰
ã€è³›æˆã€‘ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹
ã€åå¯¾ã€‘ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹

5. å…¨ä½“çš„å‚¾å‘ï¼ˆ1-2æ–‡ï¼‰
"""
        
        response = model.generate_content(prompt)
        analysis_text = response.text.strip()
        
        # çµæœã‚’ãƒ‘ãƒ¼ã‚¹
        result = {'raw_analysis': analysis_text}
        
        # æ„Ÿæƒ…åˆ†å¸ƒã®æŠ½å‡º
        agree_match = re.search(r'è³›æˆ[ï¼š:]\s*(\d+)%', analysis_text)
        oppose_match = re.search(r'åå¯¾[ï¼š:]\s*(\d+)%', analysis_text)
        neutral_match = re.search(r'ä¸­ç«‹[ï¼š:]\s*(\d+)%', analysis_text)
        
        if agree_match and oppose_match and neutral_match:
            result['sentiment'] = {
                'agree': int(agree_match.group(1)),
                'oppose': int(oppose_match.group(1)),
                'neutral': int(neutral_match.group(1))
            }
        
        # ç†±é‡ã‚¹ã‚³ã‚¢ã®æŠ½å‡º
        heat_match = re.search(r'(\d+)ç‚¹', analysis_text)
        if heat_match:
            result['heat_score'] = int(heat_match.group(1))
        
        return result
    
    except Exception as e:
        print(f"âš ï¸ æ„Ÿæƒ…åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return None


def create_discord_message(news_item: Dict, sentiment_analysis: Optional[Dict]) -> Dict:
    """
    DiscordæŠ•ç¨¿ç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
    
    Args:
        news_item: ãƒ‹ãƒ¥ãƒ¼ã‚¹é …ç›®
        sentiment_analysis: ä¸–è«–åˆ†æçµæœ
    
    Returns:
        Discordãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    title = news_item.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')
    link = news_item.get('link', '')
    source = news_item.get('source', 'ä¸æ˜')
    score = news_item.get('political_score', 0)
    
    # åŸºæœ¬ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    content = f"**ã€æ”¿æ²»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‘{title}**\n"
    content += f"ğŸ“° å‡ºå…¸: {source}\n"
    content += f"ğŸ¯ é–¢é€£åº¦: {score}ç‚¹\n"
    content += f"ğŸ”— {link}\n"
    
    # ä¸–è«–åˆ†æãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆ
    if sentiment_analysis:
        content += "\n" + "="*50 + "\n"
        content += "**ğŸ“Š ä¸–è«–åˆ†æ**\n\n"
        content += sentiment_analysis.get('raw_analysis', 'åˆ†æçµæœãªã—')
    
    return {'content': content}


def send_to_discord(message: Dict) -> bool:
    """
    Discordã«æŠ•ç¨¿
    
    Args:
        message: æŠ•ç¨¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    
    Returns:
        æˆåŠŸã—ãŸå ´åˆTrue
    """
    if not DISCORD_WEBHOOK_URL:
        print("âŒ Discord Webhook URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False
    
    try:
        response = requests.post(DISCORD_WEBHOOK_URL, json=message, timeout=10)
        response.raise_for_status()
        return True
    
    except Exception as e:
        print(f"âŒ DiscordæŠ•ç¨¿ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ğŸ›ï¸ æ”¿æ²»ãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åé›†Botï¼ˆä¸–è«–åˆ†ææ©Ÿèƒ½ä»˜ãï¼‰")
    print("=" * 60)
    print(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    if not DISCORD_WEBHOOK_URL:
        print("âŒ DISCORD_WEBHOOK_POLITICS ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        sys.exit(1)
    
    if not GEMINI_API_KEY:
        print("âŒ GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        sys.exit(1)
    
   # å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
    all_entries = []
    for source_name, feed_url in NEWS_FEEDS.items():
        print(f"ğŸ“¡ {source_name} ã‹ã‚‰å–å¾—ä¸­...")
        try:
            # User-Agentã‚’è¨­å®šã—ã¦ãƒ–ãƒ©ã‚¦ã‚¶ã¨ã—ã¦èªè­˜ã•ã›ã‚‹
            feed = feedparser.parse(
                feed_url,
                agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
            print(f"  ğŸ“Š ãƒ•ã‚£ãƒ¼ãƒ‰æƒ…å ±: status={getattr(feed, 'status', 'N/A')}, version={getattr(feed, 'version', 'N/A')}")
            print(f"  ğŸ“Š ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(feed.entries)}")
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if hasattr(feed, 'bozo') and feed.bozo:
                print(f"  âš ï¸ ãƒ•ã‚£ãƒ¼ãƒ‰è§£æã‚¨ãƒ©ãƒ¼: {feed.bozo_exception}")
            
            # ã‚¨ãƒ³ãƒˆãƒªãŒ0ä»¶ã®å ´åˆã®è©³ç´°æƒ…å ±
            if len(feed.entries) == 0:
                print(f"  âš ï¸ ã‚¨ãƒ³ãƒˆãƒªãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                print(f"  ğŸ“Š ãƒ•ã‚£ãƒ¼ãƒ‰URL: {feed_url}")
                if hasattr(feed, 'headers'):
                    print(f"  ğŸ“Š ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼: {feed.headers}")
            
            for entry in feed.entries[:20]:  # å„ã‚½ãƒ¼ã‚¹æœ€å¤§20ä»¶
                title = entry.get('title', '')
                description = entry.get('description', '') or entry.get('summary', '')
                
                if title:  # ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
                    all_entries.append({
                        'title': title,
                        'description': description,
                        'link': entry.get('link', ''),
                        'source': source_name
                    })
            print(f"  âœ… {len(feed.entries[:20])}ä»¶å–å¾—")
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
            import traceback
            print(f"  ğŸ“Š è©³ç´°: {traceback.format_exc()}")
    
    # æ”¿æ²»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    political_news = filter_political_news(all_entries)
    
    if not political_news:
        print("\næ”¿æ²»é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å‡¦ç†
    posted_count = 0
    for news in political_news[:5]:  # ä¸Šä½5ä»¶ã®ã¿å‡¦ç†
        print(f"\nå‡¦ç†ä¸­: {news['title']}")
        
        # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢
        yahoo_url = search_yahoo_news(news['title'])
        
        # ä¸–è«–åˆ†æ
        sentiment_analysis = None
        if yahoo_url:
            print(f"  âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ç™ºè¦‹: {yahoo_url}")
            comments = get_yahoo_comments(yahoo_url, max_comments=100)
            
            if comments:
                print(f"  âœ… ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—: {len(comments)}ä»¶")
                sentiment_analysis = analyze_sentiment(comments)
                time.sleep(1)  # APIåˆ¶é™å¯¾ç­–
        
        # DiscordæŠ•ç¨¿
        message = create_discord_message(news, sentiment_analysis)
        if send_to_discord(message):
            posted_count += 1
            print(f"  âœ… DiscordæŠ•ç¨¿æˆåŠŸ")
            time.sleep(2)  # Webhookåˆ¶é™å¯¾ç­–
        else:
            print(f"  âŒ DiscordæŠ•ç¨¿å¤±æ•—")
    
    print("\n" + "=" * 60)
    print(f"âœ… å®Œäº†: {posted_count}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ•ç¨¿ã—ã¾ã—ãŸ")
    print("=" * 60)


if __name__ == "__main__":
    main()
